#GTÊ≤íÊúâÈªûÂ∫ßÊ®ôÂ∞±‰∏çË®àÁÆó„ÄÅÂè™Â∞çÊåØÂπÖÂÅöÂéªÂô™
#‰ΩøÁî®DwT„ÄÅembeddingÁî®Á∑öÊÄßÂ±§„ÄÅÊäägraphÊîπÊàêcoco(Â∑≤Á∂ìËÄÉÊÖÆÂ∞çÁ®±ÁöÑÁµêÊßã)„ÄÅGCNÈÑ∞Êé•Áü©Èô£Áî®yolo
#ÈáùÂ∞ç‰∏çÂêåÈÉ®ÂàÜÁµ¶‰∫à‰∏çÂêåÊ¨äÈáç(test3ÊòØÁî®RMSEÂÄíÊï∏Ôºåtest5ÊòØÁî®RMSE)„ÄÅDwTÊîπÊàêÊ≤øËëóÊôÇÈñìÂÅöÂéªÂô™
#Ë™øÊï¥Ê¨äÈáçbetaÂÄº(beta=1)„ÄÅÊäätraining set Ë£°Èù¢ÁúãËµ∑‰æÜËàá‰∫∫È´îÂÅèÂ∑ÆÂæàÂ§öÁöÑË≥áÊñôÊãøÊéâ
#loss functionÊîπÊàêsmooth L1„ÄÅÊ¨äÈáç‰πüÁî®Êàêsmooth L1
#51Á∂≠ËΩâ17Á∂≠:51ÊääÁ∂≠ËΩâÊàê1Á∂≠ÔºåÂÜçË§áË£ΩÊàê17Á∂≠‰∏üÈÄ≤mlpgcn
#Env0~5ÂêÑÊäΩ10000Á≠ÜË®ìÁ∑¥
#ÂÅötransformer(Â∞çÂ§©Á∑öÂÅöcross attention„ÄÅ45ÂÄãÁÇ∫‰∏ÄÁµÑ)
#Èõ¢Êï£Â∞èÊ≥¢ Â±§Êï∏ÊîπÊàê2
import os
import glob
import torch
import random
import logging
import matplotlib
import numpy as np
matplotlib.use('Agg')
from tqdm import tqdm
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt

from common.utils import *
from common.camera import *
import common.eval_cal as eval_cal
from common.arguments import parse_args

from common.load_data_hm36 import Fusion
from common.load_data_3dhp import Fusion_3dhp
from common.h36m_dataset import Human36mDataset
from common.mpi_inf_3dhp_dataset import Mpi_inf_3dhp_Dataset
from torch.utils.data import ConcatDataset, DataLoader, Dataset, random_split

from model.block.refine import post_refine, refine_model
#from model.graphmlp_origin import Model
#from model.graphmlp_pca import Model
from model.graphmlp_transformer_cross import Model
from scipy.signal import butter, filtfilt
import pywt

args = parse_args()

os.environ["CUDA_VISIBLE_DEVICES"] = args.gpu

def weighted_smooth_l1_loss(predicted, target, joint_weights, beta=1.0):
    """
    predicted, target: (B, J, 2)
    joint_weights: (J,)
    beta: Smooth L1 ÁöÑÂàÜÁïåÈªû
    """
    mask = (target.abs().sum(dim=-1) > 0).float()     # (B,J)
    diff = (predicted - target).abs()                 # (B,J,2)

    smooth = torch.where(
        diff < beta,
        0.5 * (diff ** 2) / beta,
        diff - 0.5 * beta
    ).mean(dim=-1)  # (B,J) -> Âπ≥Âùá (x,y)

    weighted = smooth * joint_weights[None, :] * mask
    denom = mask.sum().clamp(min=1.0)
    return weighted.sum() / denom


def per_joint_smooth_l1(predicted, target, beta=1.0):
    """
    Ë®àÁÆóÊØèÂÄãÈóúÁØÄÁöÑÂπ≥Âùá Smooth L1 Ë™§Â∑ÆÔºåÂøΩÁï• target=(0,0)„ÄÇ
    predicted, target: (N,J,2)
    return: (J,) tensor
    """
    mask = (target.abs().sum(dim=-1) > 0)  # (N,J)
    diff = (predicted - target).abs()      # (N,J,2)

    smooth = torch.where(
        diff < beta,
        0.5 * (diff ** 2) / beta,
        diff - 0.5 * beta
    ).mean(dim=-1)  # (N,J)

    joint_smooth = torch.zeros(predicted.shape[1], device=predicted.device)
    for j in range(predicted.shape[1]):
        valid = mask[:, j]
        if valid.sum() > 0:
            joint_smooth[j] = smooth[valid, j].mean()
        else:
            joint_smooth[j] = 0.0
    return joint_smooth



def make_joint_weights(smooth_err, beta=1.0, eps=1e-6):
    """
    ‰ΩøÁî® Smooth L1 Ë™§Â∑Æ‰ΩúÁÇ∫ÊØèÂÄãÈóúÁØÄÁöÑÊ¨äÈáç‰æùÊìö„ÄÇ
    smooth_err: (J,) tensor
    """
    normed = (smooth_err + eps) / (smooth_err.mean() + eps)
    weights = normed ** beta
    weights = weights / weights.mean()
    weights = weights.clamp(min=0.5, max=2.0)
    return weights



def avg_pck_result(predicted, target, alpha=0.2, left_shoulder=6, right_hip=11):
    """
    PCK@alphaÔºåÂøΩÁï• target ÁÇ∫ (0,0) ÁöÑÈóúÁØÄÔºõËã•ËÇ©ÈªûÁº∫Â§±ÔºåË©≤Ê®£Êú¨‰∏çË®à„ÄÇ
    """
    assert predicted.shape == target.shape
    B, J, _ = predicted.shape

    # Ê®£Êú¨ÊòØÂê¶ËÇ©ÈóúÁØÄÁöÜÊúâÊïà
    ls_valid = (target[:, left_shoulder].abs().sum(dim=-1) > 0)
    rs_valid = (target[:, right_hip].abs().sum(dim=-1) > 0)
    sample_valid = ls_valid & rs_valid                     # [B]

    # ËÇ©ÂØ¨ÔºàÈÅøÂÖçÈô§ 0Ôºâ
    shoulder_dists = torch.norm(
        target[:, left_shoulder] - target[:, right_hip], dim=-1
    ).clamp(min=1e-6)                                      # [B]

    dists = torch.norm(predicted - target, dim=-1)         # [B,J]
    normalized = dists / shoulder_dists[:, None]           # [B,J]

    # ÈóúÁØÄÊúâÊïàÔºàÈùû (0,0)Ôºâ‰∏îÊ®£Êú¨ËÇ©ÂØ¨ÊúâÊïà
    joint_valid = (target.abs().sum(dim=-1) > 0) & sample_valid[:, None]  # [B,J]
    if joint_valid.sum() == 0:
        return 0.0

    correct = (normalized <= alpha) & joint_valid
    return (correct.float().sum() / joint_valid.float().sum()).item()

def train(dataloader, model, model_refine, optimizer, epoch, joint_weights=None):
    model.train()
    loss_all = {'loss': AccumLoss()}

    for i, data in enumerate(tqdm(dataloader, 0)):
        if args.dataset == 'csi':
            input_2D, gt_2D = data
            input_2D = input_2D.cuda()
            gt_2D = gt_2D.cuda()
            output_2D = model(input_2D)

            if joint_weights is None:
                # Ê≤íÊúâÊ¨äÈáç ‚Üí ÊôÆÈÄö Smooth L1
                criterion = nn.SmoothL1Loss(beta=1.0)
                loss = criterion(output_2D, gt_2D)
            else:
                # ÊúâÊ¨äÈáç ‚Üí ‰ΩøÁî®Ëá™Ë®Ç Smooth L1 Ê¨äÈáçÁâà
                loss = weighted_smooth_l1_loss(output_2D, gt_2D, joint_weights, beta=1.0)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        N = input_2D.shape[0]
        loss_all['loss'].update(loss.detach().cpu().numpy() * N, N)
        torch.cuda.empty_cache()

    return loss_all['loss'].avg


def test(actions, dataloader, model, model_refine):
    model.eval()

    if args.dataset == 'csi':
        total_pck = 0
        count = 0
        losses = []
        all_pred, all_gt = [], []

        for data in tqdm(dataloader, 0):
            input_2D, gt_2D = data
            input_2D = input_2D.cuda()
            gt_2D = gt_2D.cuda()
            output_2D = model(input_2D)

            loss = eval_cal.mpjpe(output_2D, gt_2D)
            losses.append(loss.item())

            batch_pck = avg_pck_result(output_2D, gt_2D, alpha=0.2)
            total_pck += batch_pck
            count += 1

            all_pred.append(output_2D.detach().cpu())
            all_gt.append(gt_2D.detach().cpu())

        avg_loss = sum(losses) / len(losses)
        avg_pck = total_pck / count

        # ===== Ë®àÁÆóÊØèÂÄãÈóúÁØÄ Smooth L1 Ë™§Â∑Æ =====
        all_pred = torch.cat(all_pred, dim=0)
        all_gt = torch.cat(all_gt, dim=0)
        joint_smooth = per_joint_smooth_l1(all_pred, all_gt)  # (17,)

        return avg_loss, avg_pck, joint_smooth

def save_last_ckpt(args, epoch, model, model_refine, optimizer, joint_weights):
    os.makedirs(args.checkpoint, exist_ok=True)
    ckpt = {
        "epoch": epoch,
        "model": model.state_dict(),
        "refine": model_refine.state_dict(),
        "optimizer": optimizer.state_dict(),
        "previous_best": args.previous_best,
        "joint_weights": joint_weights.detach().cpu() if joint_weights is not None else None,
    }
    torch.save(ckpt, os.path.join(args.checkpoint, "last_ckpt.pt"))


def load_last_ckpt_if_exists(args, model, model_refine, optimizer, device="cuda"):
    ckpt_path = os.path.join(args.checkpoint, "last_ckpt.pt")
    if not os.path.exists(ckpt_path):
        return 1, None  # start_epoch, joint_weights

    ckpt = torch.load(ckpt_path, map_location=device)
    model.load_state_dict(ckpt["model"])
    model_refine.load_state_dict(ckpt["refine"])
    optimizer.load_state_dict(ckpt["optimizer"])

    args.previous_best = ckpt.get("previous_best", args.previous_best)

    jw = ckpt.get("joint_weights", None)
    joint_weights = jw.to(device) if jw is not None else None

    start_epoch = ckpt["epoch"] + 1
    print(f"‚úÖ Resumed from {ckpt_path}, start_epoch={start_epoch}, previous_best={args.previous_best}")
    return start_epoch, joint_weights


if __name__ == '__main__':
    seed = 1

    random.seed(seed)
    torch.manual_seed(seed)
    np.random.seed(seed)
    torch.cuda.manual_seed_all(seed)

    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True

    if args.dataset == 'h36m':
        dataset_path = args.root_path + 'data_3d_' + args.dataset + '.npz'
        dataset = Human36mDataset(dataset_path, args)
        actions = define_actions(args.actions)

        if args.train:
            train_data = Fusion(args, dataset, args.root_path, train=True)
            train_dataloader = torch.utils.data.DataLoader(train_data, batch_size=args.batch_size,
                            shuffle=True, num_workers=int(args.workers), pin_memory=True)
        test_data = Fusion(args, dataset, args.root_path, train=False)
        test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=args.batch_size,
                            shuffle=False, num_workers=int(args.workers), pin_memory=True)
    elif args.dataset == '3dhp':
        dataset_path = args.root_path + 'data_3d_' + args.dataset + '.npz'
        dataset = Mpi_inf_3dhp_Dataset(dataset_path, args)
        actions = define_actions_3dhp(args.actions, 0)

        if args.train:
            train_data = Fusion_3dhp(args, dataset, args.root_path, train=True)
            train_dataloader = torch.utils.data.DataLoader(train_data, batch_size=args.batch_size,
                            shuffle=True, num_workers=int(args.workers), pin_memory=True)
        test_data = Fusion_3dhp(args, dataset, args.root_path, train=False)
        test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=args.batch_size,
                            shuffle=False, num_workers=int(args.workers), pin_memory=True)

    elif args.dataset == 'csi':
        
        def butter_filter(data, cutoff=10, fs=1000, order=4):
            """ Â∞çÊØèÂÄãÂ§©Á∑öË≥áÊñôÈÄ≤Ë°å Butterworth ‰ΩéÈÄöÊøæÊ≥¢ÔºàÈÄê channelÔºâ"""
            b, a = butter(order, cutoff / (0.5 * fs), btype='low')
            # data: shape (51, 6, 2025)
            filtered = np.zeros_like(data)
            for i in range(data.shape[0]):      # 51 subcarriers
                for j in range(data.shape[1]):  # 6 antennas
                    filtered[i, j] = filtfilt(b, a, data[i, j])
            return filtered
            
        def dwt_denoise(data, wavelet='db4', level=2):
            denoised = np.zeros_like(data)
            for j in range(data.shape[1]):      # 6 antennas
                for k in range(data.shape[2]):  # T subcarriers
                    signal = data[:, j, k]      # ÂèñÂá∫ (51,) ‚Üí ‰∏ÄÊ¢ùÊôÇÈñìÂ∫èÂàó
                    coeffs = pywt.wavedec(signal, wavelet, mode='periodization', level=level)

                    # Ë®àÁÆóÁµ±Ë®àÈñæÂÄº
                    sigma = np.median(np.abs(coeffs[-1])) / 0.6745
                    uthresh = sigma * np.sqrt(2 * np.log(len(signal)))

                    # Â∞çÁ¥∞ÁØÄ‰øÇÊï∏ÂÅöËªüÈñæÂÄºËôïÁêÜ
                    coeffs[1:] = [
                        pywt.threshold(c, value=uthresh, mode='soft') 
                        if np.max(np.abs(c)) > 1e-12 else np.zeros_like(c) 
                        for c in coeffs[1:]
                    ]

                    # ÈáçÂª∫
                    recon = pywt.waverec(coeffs, wavelet, mode='periodization')
                    denoised[:, j, k] = recon[:signal.shape[0]]
            return denoised


        def normalize(data):
            """ Â∞çÊØèÂÄã channel zero-mean, unit-std """
            mean = np.mean(data, axis=2, keepdims=True)
            std = np.std(data, axis=2, keepdims=True) + 1e-8
            return (data - mean) / std

        class CSI_KPT_Dataset(Dataset):
            def __init__(self, env_roots, per_env_samples=10000, seed=1, use_dwt_folder="npy",
                        kpt_folder="kpt_npz_vitpose"):
                self.pairs = []  # list of (csi_path, kpt_path)

                rng = random.Random(seed)

                for env_path in env_roots:
                    env_name = os.path.basename(env_path.rstrip("/"))
                    csi_root = os.path.join(env_path, use_dwt_folder)
                    kpt_root = os.path.join(env_path, kpt_folder)

                    csi_files = glob.glob(os.path.join(csi_root, "**", "*.npz"), recursive=True)
                    kpt_files = glob.glob(os.path.join(kpt_root, "**", "*.npz"), recursive=True)

                    kpt_map = {os.path.splitext(os.path.basename(f))[0]: f for f in kpt_files}

                    env_pairs = []
                    for csi_path in csi_files:
                        timestamp = os.path.splitext(os.path.basename(csi_path))[0]
                        if timestamp in kpt_map:
                            env_pairs.append((csi_path, kpt_map[timestamp]))

                    # --- ÊØèÂÄã Env ÊäΩÊ®£ ---
                    if len(env_pairs) == 0:
                        print(f"‚ö†Ô∏è {env_name}: no matched pairs")
                        continue

                    if len(env_pairs) > per_env_samples:
                        env_pairs = rng.sample(env_pairs, per_env_samples)
                        print(f"‚úÖ {env_name}: sampled {per_env_samples}/{len(csi_files)} (matched {len(env_pairs)})")
                    else:
                        print(f"‚úÖ {env_name}: only {len(env_pairs)} matched (<{per_env_samples}), use all")

                    self.pairs.extend(env_pairs)

                rng.shuffle(self.pairs)
                print(f"üìå Total pairs: {len(self.pairs)}")

            def __len__(self):
                return len(self.pairs)

            def __getitem__(self, idx):
                csi_path, kpt_path = self.pairs[idx]

                csi_npz = np.load(csi_path)
                mag = csi_npz["mag"].astype(np.float32)
                pha = csi_npz["pha"].astype(np.float32)

                # ‰Ω†Ë™™„ÄåÂè™Â∞çÊåØÂπÖÂÅöÂéªÂô™„ÄçÔºöÂ∞±Êää mag ÂÅö dwt/normalizeÔºåpha Áõ¥Êé• normalize Êàñ‰∏çÂãï
                mag = normalize(dwt_denoise(mag))
                pha = normalize(pha)  # ÊàñËÄÖ‰∏ç normalizeÔºåÁúã‰Ω†Ë≥áÊñôÂ∞∫Â∫¶

                csi = np.concatenate([mag, pha], axis=2)

                kpt_npz = np.load(kpt_path)
                keypoints = kpt_npz["keypoints"].astype(np.float32)

                if keypoints.shape[0] > 1:
                    keypoints = keypoints[0]
                elif keypoints.shape == (1, 17, 2):
                    keypoints = keypoints[0]
                elif keypoints.shape != (17, 2):
                    keypoints = np.zeros((17, 2), dtype=np.float32)

                return torch.tensor(csi), torch.tensor(keypoints)
        # ÂÆöÁæ©ÊâÄÊúâ Env Ë≥áÊñôÂ§æË∑ØÂæë
        #env_roots = [
        #    os.path.join(args.root_path, f"Env{i}") for i in range(6)
        #]
        env_roots = [os.path.join(args.root_path, f"Env{i}_train") for i in range(6)]  # Env0~Env5
        full_dataset = CSI_KPT_Dataset(env_roots, per_env_samples=10000, seed=1)

        full_dataset = CSI_KPT_Dataset(env_roots)

        # ÊãÜÂàÜË≥áÊñôÈõÜ
        train_size = int(0.8 * len(full_dataset))
        test_size = len(full_dataset) - train_size
        train_data, test_data = random_split(full_dataset, [train_size, test_size])

        # DataLoader
        train_dataloader = DataLoader(train_data, batch_size=args.batch_size, shuffle=True,
                                    num_workers=int(args.workers), pin_memory=True)
        test_dataloader = DataLoader(test_data, batch_size=args.batch_size, shuffle=False,
                                    num_workers=int(args.workers), pin_memory=True)

        actions = ['csi']

    model = Model(args).cuda()
    #model = Model(args, pca_path="pca_24300_to_512.npz").cuda()
    model_refine = post_refine(args).cuda()
    print(next(model.parameters()).device)

    if args.previous_dir != '':
        Load_model(args, model, model_refine)

    lr = args.lr
    all_param = []
    all_param += list(model.parameters())

    if args.refine:
        all_param += list(model_refine.parameters())

    optimizer = optim.Adam(all_param, lr=lr, amsgrad=True)
    start_epoch, joint_weights = load_last_ckpt_if_exists(
        args, model, model_refine, optimizer, device="cuda"
    )
    
    ##--------------------------------epoch-------------------------------- ##
    best_epoch = 0
    loss_epochs = []
    mpjpes = []

    joint_weights = None  # ÂàùÂßãÂåñÔºåÁ¨¨‰∏ÄËº™‰∏çÂä†Ê¨ä

    for epoch in range(start_epoch, args.nepoch + 1):
        ## train
        if args.train:
            save_last_ckpt(args, epoch, model, model_refine, optimizer, joint_weights)
            loss = train(train_dataloader, model, model_refine, optimizer, epoch, joint_weights)
            loss_epochs.append(loss * 1000)

        ## test
        with torch.no_grad():
            if args.dataset == 'csi':
                p1, PCK, joint_smooth = test(actions, test_dataloader, model, model_refine)
                mpjpes.append(p1)

                # Êõ¥Êñ∞ joint_weights
                joint_weights = make_joint_weights(joint_smooth.cuda())
                print(f"Epoch {epoch} Joint SmoothL1: {joint_smooth.cpu().numpy()}")
                print(f"Epoch {epoch} Joint Weights: {joint_weights.cpu().numpy()}")
            else:
                p1, p2, pck, auc = test(actions, test_dataloader, model, model_refine)
                mpjpes.append(p1)

        ## save the best model
        if args.train and p1 < args.previous_best:
            best_epoch = epoch
            args.previous_name = save_model(args, epoch, p1, model, 'model')

            if args.refine:
                args.previous_refine_name = save_model(args, epoch, p1, model_refine, 'refine')

            args.previous_best = p1

        ## print
        if args.train:
            logging.info('epoch: %d, lr: %.6f, Train loss: %.4f, mpjpe: %.2f, PCK: %.2f' % (epoch, lr, loss, p1, PCK))
            print('%d, lr: %.6f, Train loss: %.4f, RMSE: %.2f, PCK: %.2f' % (epoch, lr, loss, p1, PCK))
            
            
            ## adjust lr
            if epoch % args.lr_decay_epoch == 0:
                lr *= args.lr_decay_large
                for param_group in optimizer.param_groups:
                    param_group['lr'] *= args.lr_decay_large
            else:
                lr *= args.lr_decay
                for param_group in optimizer.param_groups:
                    param_group['lr'] *= args.lr_decay 
            '''
            
            # ÊØè lr_decay_epoch ÂÄã epoch Êõ¥Êñ∞‰∏ÄÊ¨°Â≠∏ÁøíÁéá
            if epoch % args.lr_decay_epoch == 1 and epoch > 1:
                lr *= args.lr_decay
                for param_group in optimizer.param_groups:
                    param_group['lr'] = lr
            '''
            
        else:
            if args.dataset == 'h36m':
                print('p1: %.2f, p2: %.2f' % (p1, p2))
            elif args.dataset == '3dhp':
                print('pck: %.2f, auc: %.2f, p1: %.2f, p2: %.2f' % (pck, auc, p1, p2))
            break

        ## training curves
        if epoch == 1:
            start_epoch = 3
                
        if args.train and epoch > start_epoch:
            plt.figure()
            epoch_x = np.arange(start_epoch+1, len(loss_epochs)+1)
            plt.plot(epoch_x, loss_epochs[start_epoch:], '.-', color='C0')
            plt.plot(epoch_x, mpjpes[start_epoch:], '.-', color='C1')
            plt.legend(['Loss', 'Test'])
            plt.ylabel('MPJPE')
            plt.xlabel('Epoch')
            plt.xlim((start_epoch+1, len(loss_epochs)+1))
            plt.savefig(os.path.join(args.checkpoint, 'loss.png'))
            plt.close()
